{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 11\n",
    "In this experiment, we're back to adversarial training, but trying the thresholding method. Judging the points using the loss from the validation set as a threshold after multiplying with a certain factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MNIST\t\t\t       SWaT_Dataset_Normal_v1.xlsx\n",
      " SWaT_Dataset_Attack_v0.csv    WADI_14days_new.csv\n",
      " SWaT_Dataset_Attack_v0.xlsx  'WADI.A1_9 Oct 2017'\n",
      " SWaT_Dataset_Normal_v0.csv   'WADI.A2_19 Nov 2019'\n",
      " SWaT_Dataset_Normal_v0.xlsx   WADI_attackdataLABLE.csv\n",
      " SWaT_Dataset_Normal_v1.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../../Projects/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from layers import *\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve, roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Projects/data/SWaT_Dataset_Normal_v1.csv')\n",
    "df = df.drop(columns=[' Timestamp', 'Normal/Attack'])\n",
    "df = df.astype('float64')\n",
    "mm = StandardScaler()\n",
    "Normalized = pd.DataFrame(mm.fit_transform(df))\n",
    "train_set = Normalized[: int(0.8 * Normalized.shape[0])]\n",
    "validation_set = Normalized[int(0.8 * Normalized.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "\n",
    "train_dataset = SWat_dataset(train_set, train_set, window_size, device)\n",
    "validation_dataset = SWat_dataset(validation_set, validation_set, window_size, device)\n",
    "\n",
    "batch_size = 4096\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEncoder(nn.Module):\n",
    "    def __init__(self, num_nodes, window_size, alpha, k, device):\n",
    "        super(GEncoder, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.window_size = window_size\n",
    "        self.conv1 = GCNLayer(window_size, 33)\n",
    "        self.conv2 = GCNLayer(33, 10)\n",
    "        self.idx = torch.arange(num_nodes).to(device)\n",
    "        self.A = Graph_Directed_A(num_nodes, window_size, alpha, k, device)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = torch.transpose(X, 1, 2)\n",
    "        adj = self.A(self.idx)\n",
    "        h = self.conv1(adj, X).relu()\n",
    "        h = self.conv2(adj, h).relu()\n",
    "        return h\n",
    "        \n",
    "        \n",
    "\n",
    "class GCN_autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, num_nodes, window_size):\n",
    "        super(GCN_autoencoder, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.encoder = encoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(510, 1200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1200, num_nodes * window_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        x = self.encoder(X)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.decoder(x)\n",
    "        return x.view(-1, self.window_size, self.num_nodes)\n",
    "\n",
    "    def get_adj(self):\n",
    "        return self.encoder.A(self.encoder.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 51 # number of nodes\n",
    "alpha = 0.1 # hyperparameter for weights of edges\n",
    "k = None  # max number of edges for each node\n",
    "epochs = 200\n",
    "out_channels = 2 # number of process states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = GEncoder(num_nodes, window_size, alpha, k, device)\n",
    "AE1 = GCN_autoencoder(Encoder, num_nodes, window_size)\n",
    "AE2 = GCN_autoencoder(Encoder, num_nodes, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN_autoencoder(\n",
       "  (encoder): GEncoder(\n",
       "    (conv1): GCNLayer(\n",
       "      (dense): Linear(in_features=100, out_features=33, bias=True)\n",
       "    )\n",
       "    (conv2): GCNLayer(\n",
       "      (dense): Linear(in_features=33, out_features=10, bias=True)\n",
       "    )\n",
       "    (A): Graph_ReLu_W()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=510, out_features=1200, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1200, out_features=2500, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=2500, out_features=5100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE1.to(device)\n",
    "AE2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adam(AE1.parameters())\n",
    "optimizer2 = torch.optim.Adam(AE2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 ---> Val loss: AE1 0.1132, AE2: 0.1141\n",
      "Train loss: AE1 0.2740, AE2 0.2647\n",
      "Epoch: 1 ---> Val loss: AE1 0.1061, AE2: -0.0136\n",
      "Train loss: AE1 0.2505, AE2 -0.1368\n",
      "Epoch: 2 ---> Val loss: AE1 0.0971, AE2: -0.0460\n",
      "Train loss: AE1 0.2708, AE2 -0.2196\n",
      "Epoch: 3 ---> Val loss: AE1 0.0906, AE2: -0.0573\n",
      "Train loss: AE1 0.2858, AE2 -0.2538\n",
      "Epoch: 4 ---> Val loss: AE1 0.1704, AE2: -0.1213\n",
      "Train loss: AE1 0.4687, AE2 -0.3160\n",
      "Epoch: 5 ---> Val loss: AE1 0.1218, AE2: -0.0927\n",
      "Train loss: AE1 0.3862, AE2 -0.3407\n",
      "Epoch: 6 ---> Val loss: AE1 0.1014, AE2: -0.0817\n",
      "Train loss: AE1 0.3645, AE2 -0.3408\n",
      "Epoch: 7 ---> Val loss: AE1 0.1100, AE2: -0.0928\n",
      "Train loss: AE1 0.3682, AE2 -0.3493\n",
      "Epoch: 8 ---> Val loss: AE1 0.1055, AE2: -0.0910\n",
      "Train loss: AE1 0.3710, AE2 -0.3564\n",
      "Epoch: 9 ---> Val loss: AE1 0.1197, AE2: -0.1056\n",
      "Train loss: AE1 0.3863, AE2 -0.3705\n",
      "Epoch: 10 ---> Val loss: AE1 0.1850, AE2: -0.1608\n",
      "Train loss: AE1 0.4085, AE2 -0.3793\n",
      "Epoch: 11 ---> Val loss: AE1 0.1399, AE2: -0.1249\n",
      "Train loss: AE1 0.4502, AE2 -0.4234\n",
      "Epoch: 12 ---> Val loss: AE1 0.3271, AE2: -0.3030\n",
      "Train loss: AE1 0.4334, AE2 -0.4113\n",
      "Epoch: 13 ---> Val loss: AE1 0.4299, AE2: -0.3927\n",
      "Train loss: AE1 0.8245, AE2 -0.7918\n",
      "Epoch: 14 ---> Val loss: AE1 0.4473, AE2: -0.4186\n",
      "Train loss: AE1 0.8415, AE2 -0.7934\n",
      "Epoch: 15 ---> Val loss: AE1 0.4052, AE2: -0.3915\n",
      "Train loss: AE1 0.8278, AE2 -0.8078\n",
      "Epoch: 16 ---> Val loss: AE1 0.4254, AE2: -0.4038\n",
      "Train loss: AE1 0.8506, AE2 -0.8014\n",
      "Epoch: 17 ---> Val loss: AE1 0.4254, AE2: -0.4119\n",
      "Train loss: AE1 0.8315, AE2 -0.8049\n",
      "Epoch: 18 ---> Val loss: AE1 0.4082, AE2: -0.3971\n",
      "Train loss: AE1 0.8289, AE2 -0.8163\n",
      "Epoch: 19 ---> Val loss: AE1 0.4071, AE2: -0.3977\n",
      "Train loss: AE1 0.8287, AE2 -0.8195\n"
     ]
    }
   ],
   "source": [
    "AE1_val_history = []\n",
    "AE2_val_history = []\n",
    "for i in range(epochs):\n",
    "    running_loss_AE1 = []\n",
    "    running_loss_AE2 = []\n",
    "    val_loss_AE1 = []\n",
    "    val_loss_AE2 = []\n",
    "    for index_b, features in enumerate(train_loader):\n",
    "        \n",
    "        w1 = AE1(features)\n",
    "        w2 = AE2(features)\n",
    "        w3 = AE2(w1)\n",
    "        lossAE1 = (1 / (i + 1)) * torch.mean((features - w1) ** 2) + (1 - (1 / (i + 1))) * torch.mean((features - w3) ** 2)\n",
    "        \n",
    "        running_loss_AE1.append(lossAE1)\n",
    "        lossAE1.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "        \n",
    "        w1 = AE1(features)\n",
    "        w2 = AE2(features)\n",
    "        w3 = AE2(w1)\n",
    "        lossAE2 = (1 / (i + 1)) * torch.mean((features - w2) ** 2) - (1 - (1 / (i + 1))) * torch.mean((features - w3) ** 2)\n",
    "        \n",
    "        running_loss_AE2.append(lossAE2)\n",
    "        lossAE2.backward()\n",
    "        optimizer2.step()\n",
    "        optimizer2.zero_grad()\n",
    "    \n",
    "    for index_b, features in enumerate(validation_loader):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            w1 = AE1(features)\n",
    "            w2 = AE2(features)\n",
    "            w3 = AE2(w1)\n",
    "            lossAE1 = (1 / (i + 1)) * torch.mean((features - w1) ** 2) + (1 - (1 / (i + 1))) * torch.mean((features - w3) ** 2)\n",
    "            lossAE2 = (1 / (i + 1)) * torch.mean((features - w2) ** 2) - (1 - (1 / (i + 1))) * torch.mean((features - w3) ** 2)\n",
    "            val_loss_AE1.append(lossAE1)\n",
    "            val_loss_AE2.append(lossAE2)\n",
    "    AE1_val_history.append(torch.stack(val_loss_AE1).mean().item())\n",
    "    AE2_val_history.append(torch.stack(val_loss_AE2).mean().item())\n",
    "    print(f'Epoch: {i} ---> Val loss: AE1 {AE1_val_history[-1]:.4f}, AE2: {AE2_val_history[-1]:.4f}')\n",
    "    print(f'Train loss: AE1 {torch.stack(running_loss_AE1).mean().item():.4f}, AE2 {torch.stack(running_loss_AE2).mean().item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../../Projects/data/SWaT_Dataset_Attack_v0.csv')\n",
    "labels = df2['Normal/Attack']\n",
    "df2 = df2.drop(columns=[' Timestamp', 'Normal/Attack'])\n",
    "df2 = df2.astype('float64')\n",
    "df2.columns = df.columns\n",
    "test_normalized = pd.DataFrame(mm.transform(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SWat_dataset(test_normalized, test_normalized, window_size, device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(test_loader, alpha=.5, beta=.5):\n",
    "    results=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            w1=AE1(batch)\n",
    "            w2=AE2(w1)\n",
    "            results.append(alpha * torch.mean((batch-w1)**2, axis=1) +\n",
    "                           beta * torch.mean((batch-w2)**2, axis=1))\n",
    "    return results\n",
    "\n",
    "def get_threshold(val_loader):\n",
    "    recon_errors = testing(val_loader)\n",
    "    complete_vals = np.concatenate([torch.stack(recon_errors[:-1]).view(-1, num_nodes).detach().cpu().numpy(),\n",
    "                                    recon_errors[-1].view(-1, num_nodes).detach().cpu().numpy()])\n",
    "    return np.max(complete_vals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.values\n",
    "labels = [0 if (lab == 'Normal') else 1 for lab in labels]\n",
    "windows_labels=[]\n",
    "for i in range(len(labels)-window_size):\n",
    "    windows_labels.append(list(np.int32(labels[i:i+window_size])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [1.0 if (np.sum(window) > 0) else 0 for window in windows_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = get_threshold(validation_loader)\n",
    "results=testing(test_loader, alpha=0.3, beta=0.7)\n",
    "y_pred=np.concatenate([torch.stack(results[:-1]).view(-1, num_nodes).detach().cpu().numpy(), \n",
    "                       results[-1].view(-1, num_nodes).detach().cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.784568523469343\n",
      "Recall: 0.617271818213165\n",
      "F1 Score: 0.6909375120614458\n",
      "TP: 35803\n",
      "TN: 381986\n",
      "FP: 9831\n",
      "FN: 22199\n",
      "Factor is: 230\n"
     ]
    }
   ],
   "source": [
    "factor = 230\n",
    "res = y_pred > (thresholds * factor)\n",
    "boo = np.any(res, axis = 1)\n",
    "verdicts = [1 if elem else 0 for elem in boo]\n",
    "conf_matrix = confusion_matrix(y_test, verdicts)\n",
    "TP = conf_matrix[1, 1]\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "F1 = f1_score(y_test, verdicts)\n",
    "precision = precision_score(y_test, verdicts)\n",
    "recall = recall_score(y_test, verdicts)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", F1)\n",
    "print(f'TP: {TP}\\nTN: {TN}\\nFP: {FP}\\nFN: {FN}')\n",
    "print(f'Factor is: {factor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(AE1.state_dict(), 'GAE_77_Relu_50_epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE1.load_state_dict(torch.load('GAE_77_Relu_50_epochs.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NIS2",
   "language": "python",
   "name": "art2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
